return {
  "huggingface/llm.nvim",
  opts = {
    backend = "ollama",
    model = "qwen2.5-coder:1.5b",
    url = "http://127.0.0.1:11434", -- llm-ls uses "/api/generate"
    -- cf https://github.com/ollama/ollama/blob/main/docs/api.md#parameters
    tokens_to_clear = { "<|endoftext|>" },
    fim = {
      enabled = true,
      prefix = "<|fim_prefix|>",
      middle = "<|fim_middle|>",
      suffix = "<|fim_suffix|>",
    },
    tokenizer = {
      repository = "Qwen/Qwen2.5-Coder-1.5B-Instruct",
    },
    request_body = {
      options = {
        temperature = 0.2,
        top_p = 0.95,
      },
    },
    context_window = 32768,
    lsp = {
      bin_path = vim.api.nvim_call_function("stdpath", { "data" }) .. "/mason/bin/llm-ls",
    },
  },
}
